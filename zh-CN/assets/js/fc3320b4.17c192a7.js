"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1296],{18379:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llm_ggml_model-fdd30b6f6640c4def399eef562ac03ba.png"},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var o=t(96540);const i={},l=o.createContext(i);function r(e){const n=o.useContext(l);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(l.Provider,{value:n},e.children)}},34056:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llm_chat_result-be8e98ff482447100ea379b3f32f625f.png"},59623:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llm_hugging_face-cbdf155bf78c91051ef4b92c614e786f.png"},76162:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"deploy/install_llm","title":"LLM Local Deployment","description":"The users have the capability to locally deploy extensive models as a service. The complete process, encompassing downloading pre-trained models, deploying them as a service, and debugging, is described in the following steps. It is essential for the user\'s machine to have Docker installed and be granted access to the repository containing these large models.","source":"@site/versions/version-current/docs-en/source/7.deploy/5.install_llm.md","sourceDirName":"7.deploy","slug":"/deploy/install_llm","permalink":"/geaflow-website/zh-CN/docs/deploy/install_llm","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"\ud83c\udf08 G6VP Graph Visualization","permalink":"/geaflow-website/zh-CN/docs/deploy/collaborate_with_g6vp"},"next":{"title":"Install Minikube","permalink":"/geaflow-website/zh-CN/docs/deploy/install_minikube"}}');var i=t(74848),l=t(28453);const r={},s="LLM Local Deployment",a={},d=[{value:"Step 1: Download the Model File",id:"step-1-download-the-model-file",level:2},{value:"Step 2: Prepare the Docker Container Environment",id:"step-2-prepare-the-docker-container-environment",level:2},{value:"Step 3: Model Service Deployment",id:"step-3-model-service-deployment",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",...(0,l.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"llm-local-deployment",children:"LLM Local Deployment"})}),"\n",(0,i.jsx)(n.p,{children:"The users have the capability to locally deploy extensive models as a service. The complete process, encompassing downloading pre-trained models, deploying them as a service, and debugging, is described in the following steps. It is essential for the user's machine to have Docker installed and be granted access to the repository containing these large models."}),"\n",(0,i.jsx)(n.h2,{id:"step-1-download-the-model-file",children:"Step 1: Download the Model File"}),"\n",(0,i.jsxs)(n.p,{children:["The pre-trained large model file has been uploaded to the ",(0,i.jsx)(n.a,{href:"https://huggingface.co/tugraph/CodeLlama-7b-GQL-hf",children:"Hugging Face repository"}),". Please proceed with downloading and locally unzipping the model file.\n",(0,i.jsx)(n.img,{alt:"hugging",src:t(59623).A+"",width:"3122",height:"1776"})]}),"\n",(0,i.jsx)(n.h2,{id:"step-2-prepare-the-docker-container-environment",children:"Step 2: Prepare the Docker Container Environment"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Run the following command on the terminal to download the Docker image required for model servicing:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"docker pull tugraph/llam_infer_service:0.0.1\n\n// Use the following command to verify that the image was successfully downloaded\n\ndocker images\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"Run the following command to start the Docker container:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"docker run -it  --name ${Container name} -v ${Local model path}:${Container model path} -p ${Local port}:${Container service port} -d ${Image name}\n\n// Such as\ndocker run -it --name my-model-container -v /home/huggingface:/opt/huggingface -p 8000:8000 -d llama_inference_server:v1\n\n// Check whether the container is running properly\ndocker ps\n"})}),"\n",(0,i.jsx)(n.p,{children:"Here, we map the container's port 8000 to the local machine's port 8000, mount the directory where the local model (/home/huggingface) resides to the container's path (/opt/huggingface), and set the container name to my-model-container."}),"\n",(0,i.jsx)(n.h2,{id:"step-3-model-service-deployment",children:"Step 3: Model Service Deployment"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Model transformation"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"// Enter the container you just created\ndocker exec -it ${container_id} bash\n\n// Execute the following command\ncd /opt/llama_cpp\npython3 ./convert.py ${Container model path}\n"})}),"\n",(0,i.jsxs)(n.p,{children:["When the execution is complete, a file with the prefix ggml-model is generated under the container model path.\n",(0,i.jsx)(n.img,{alt:"undefined",src:t(18379).A+"",width:"1994",height:"184"})]}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"Model quantization (optional)\nTake the llam2-7B model as an example: By default, the accuracy of the model converted by convert.py is F16 and the model size is 13.0GB. If the current machine resources cannot satisfy such a large model inference, the converted model can be further quantized by./quantize."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"// As shown below, q4_0 quantizes the original model to int4 and compresses the model size to 3.5GB\n\ncd /opt/llama_cpp\n./quantize ${Default generated F16 model path} ${Quantized model path} q4_0\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The following are reference indicators such as the size and reasoning speed of the quantized model:\n",(0,i.jsx)(n.img,{alt:"undefined",src:t(90966).A+"",width:"1374",height:"850"})]}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsx)(n.li,{children:"Model servicing\nRun the following command to deploy the above generated model as a service, and specify the address and port of the service binding through the parameters:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"// ./server -h. You can view parameter details\n// ${ggml-model...file} The file name prefixes the generated ggml-model\n\ncd /opt/llama_cpp\n./server --host ${ip} --port ${port} -m ${Container model path}/${ggml-model...file} -c 4096\n\n// Such as\n./server --host 0.0.0.0 --port 8000 -m  /opt/huggingface/ggml-model-f16.gguf -c 4096\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"4",children:["\n",(0,i.jsx)(n.li,{children:'Debugging service\nSend an http request to the service address, where "prompt" is the query statement and "content" is the inference result.'}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'curl --request POST \\\n    --url http://127.0.0.1:8000/completion \\\n    --header "Content-Type: application/json" \\\n    --data \'{"prompt": "\u8bf7\u8fd4\u56de\u5c0f\u7ea2\u768410\u4e2a\u5e74\u9f84\u5927\u4e8e20\u7684\u670b\u53cb","n_predict": 128}\'\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Debugging service\nThe following is the model inference result after service deployment:\n",(0,i.jsx)(n.img,{alt:"undefined",src:t(34056).A+"",width:"3452",height:"378"})]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},90966:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llm_quantization_table-adc2d8cd29b0e371a75ccf6e3874053d.png"}}]);